{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzkOAuU1N0XqMd8D93BYRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiraag-kakar/ML/blob/master/Day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIPOd11S4GSq",
        "colab_type": "text"
      },
      "source": [
        "# **Data Pre Processing**\n",
        "## Step 1 : Importing the required libraries\n",
        "Usually Numpy which contains mathematical functions and Pandas which is used to import and manage the datasets.\n",
        "## Step 2 : Importing the dataset\n",
        "Datasets are generally available in csv format and we use read_csv method to read a local csv file as a dataframe.\n",
        "Then we make separate matrix and vector of independent and dependent variables from the dataframe.\n",
        "## Step 3 : Handling the missing data\n",
        "As missing data points can affect the model's performance we generally replace them by mean or median of entire column.\n",
        "## Step 4 : Encoding the categorical data\n",
        "Categorical data are variables that contain label values rather than numeric values. The number of possible values is often limited to a fixed set. Example values such as 'Yes' or 'No' cannot be used in mathematical equations thus we encode these variable into numbers.\n",
        "## Step 5 : Splitting the data into training and testing set\n",
        "The split is generally 80/20.\n",
        "## Step 6 : Feature scaling\n",
        "Most of the machine learning models use the euclidean distance b/w two data points in their computations. Features highly varying in magnitudes, units and range pose problems.\n",
        "\n",
        "\n",
        "High magnitude features will weigh more in the distance calculations than features with low magnitudes.\n",
        "\n",
        "\n",
        "\n",
        "Done by feature standardization or Z-Score Normalization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnV_TOBJ4AUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2\n",
        "dataset = pd.read_csv('Data.csv')\n",
        "X = dataset.iloc[ : , :-1].values\n",
        "Y = dataset.iloc[ : , 3].values\n",
        "\n",
        "# Step 3\n",
        "from sklearn.preprocessing import Imputer\n",
        "imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)\n",
        "imputer = imputer.fit(X[ : , 1:3])\n",
        "X[ : , 1:3] = imputer.transform(X[ : , 1:3])\n",
        "\n",
        "\n",
        "# Step 4\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X = LabelEncoder()\n",
        "X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])\n",
        "\n",
        "# Creating Dummy Variable\n",
        "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
        "X = onehotencoder.fit_transform(X).toarray()\n",
        "labelencoder_Y = LabelEncoder()\n",
        "Y =  labelencoder_Y.fit_transform(Y)\n",
        "\n",
        "# Step 5\n",
        "from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Step 6\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}